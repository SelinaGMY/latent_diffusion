from myosuite.utils import gym
import skvideo
# skvideo.setFFmpegPath('/opt/homebrew/bin/ffmpeg')
# skvideo.setFFmpegPath('/opt/homebrew/Cellar/ffmpeg')
import skvideo.io
# print(os.environ['PATH'])
from IPython.display import HTML
from base64 import b64encode
import jax.numpy as jnp
from jax.experimental.host_callback import id_print
from jax.scipy.special import logsumexp
from jax.lax import scan
from jax import grad, jit, vmap, lax
import jax.random as random
rng = random.PRNGKey(2022)
from functools import partial
import optax
from flax.training import checkpoints, train_state
from flax import linen as nn

import numpy as np
from typing import Any, Tuple
import pickle
import matplotlib.pyplot as plt
from model.temporal import temporal_unet
from model.z_posterior import dynamics, precoder, MLP_Gaussian, infer

# diffusion with states and actions
# model_type = 'baseline' # states+raw 6-d action
# model_type = 'empowerment' # states+1-d action, with 6-d action created by synergy vector
# model_type = 'synergy_manifold' # states+6-d action generated by synergy vector
# model_type = 'VAE' # states+VAE encoded 1-d action, with 6-d action created by VAE decoder
# model_type = 'VAE_6d' # states+6-d action generated by VAE

# diffusion with states only
# model_type = 'state_only' # generate 1-d action by posterior q(z|s,y') and 6-d action by synergy vector
model_type = 'inverse_dynamics' # generate 6-d action by inverse dynamics p(a|s,s')

import os 
os.makedirs(model_type +  '/MPC/videos_z', exist_ok=True)

from config import posterior_args, VAE_args, inverse_dynamics_args

beta_min = 0.1
beta_max = 20

def beta_t(t):
    return beta_min + t*(beta_max - beta_min)

def alpha_t(t):
    return t * beta_min + 0.5 * t**2 * (beta_max - beta_min)

def drift(x, t):
    return -0.5 * beta_t(t)[:,None,None] * x

def dispersion(t):
    return jnp.sqrt(beta_t(t))

def mean_factor(t):
    return jnp.exp(-0.5 * alpha_t(t))

def var(t):
    return 1 - jnp.exp(-alpha_t(t))

R = 100
train_ts = jnp.linspace(1, 1e-3, R)

@partial(jit, static_argnums=[1,2,3,4,5,11])
def reverse_sde(rng, N, n_samples, forward_drift, dispersion, score, params_s, train_ts, snr, alphas, s0, transition_dim):

    def f(snr, s0, carry, params):
        noise, t, alpha = params
        x, rng, dt = carry
        rng, step_rng = random.split(rng)

        t = jnp.ones((x.shape[0], )) * t

        # corrector
        grad = score(params_s, x, t * 999)
        grad_norm = jnp.linalg.norm(grad.reshape((grad.shape[0], -1)), axis=-1).mean()

        rng, step_rng = random.split(rng)
        corrector_noise = random.normal(step_rng, x.shape)
        noise_norm = jnp.sqrt(jnp.prod(jnp.array(x.shape[1:])))

        step_size = (snr * noise_norm / grad_norm) ** 2 * 2 * alpha

        x_mean = x + step_size * grad
        x = x_mean + corrector_noise * jnp.sqrt(step_size * 2)
        # overwrite/condition on/observe/ initial time point
        x = x.at[:,0,:9].set(s0)

        # predictor
        disp = dispersion(t[0])
        drift = -forward_drift(x, t) + disp**2 * score(params_s, x, t * 999) # / jnp.sqrt(var(t[0]))
        x = x + dt * drift + jnp.sqrt(dt)*disp*noise

        # overwrite/condition on/observe/ initial time point
        x = x.at[:,0,:9].set(s0)

        return (x, rng, dt), (x)

    rng, step_rng = random.split(rng)
    initial = random.normal(step_rng, (n_samples, N, transition_dim))
    # overwrite/condition on/observe/ initial time point
    initial = initial.at[:,0,:9].set(s0)
    dts = train_ts[0] - train_ts[1]
    noises = random.normal(rng, (train_ts.shape[0], n_samples, N, transition_dim))
    noises = noises.at[-1,:,:,:].set(0.) # set noise at end of loop to 0
    params = (noises, train_ts, alphas[::-1])
    (x, _, _), x_traj = scan(partial(f, snr, s0), (initial, rng, dts), params)
    return x, x_traj

class Args:
    N_epochs = 50_000
    batch_size = 200
    early_stop_start = 150_000
    train_ratio = 0.8
    seed = 111
    horizon = 4  # time steps
    transition_dim = 10  # spatial dimensionality
    u_net_dim = 32
    u_net_dim_mults = [1, 4, 8]
    u_net_attention = True
    learning_rate = 2e-4
    adam_b1 = 0.9
    adam_b2 = 0.999
    adam_eps = 1e-8
    weight_decay = 0 # 0.0001
    max_grad_norm = 1
    ema_decay = 0.
    gamma = 0.997
    h_dims_dynamics = [256,256]
    control_indx = [0]

args = Args()

if model_type == 'state_only' or model_type == 'inverse_dynamics':
    args.transition_dim = 9
elif model_type == 'empowerment' or model_type == 'VAE':
    args.transition_dim = 10
elif model_type == 'synergy_manifold' or model_type == 'baseline' or model_type == 'VAE_6d':
    args.transition_dim = 15

if model_type == 'state_only' or model_type == 'inverse_dynamics':
    folder_name = 'state_only'
else:
    folder_name = model_type

with open(folder_name + "/diffusion/params_score_0912.pickle", "rb") as input_file:
    params_s = pickle.load(input_file)

score_model = temporal_unet(args)
optimizer_s = optax.chain(optax.adamw(learning_rate = args.learning_rate,
                    b1 = args.adam_b1,
                    b2 = args.adam_b2,
                    eps = args.adam_eps,
                    weight_decay = args.weight_decay),
                    optax.clip_by_global_norm(args.max_grad_norm),
                    optax.ema(decay = args.ema_decay))
state_s = train_state.TrainState.create(apply_fn = score_model.apply, params = params_s['params'], tx = optimizer_s)

# predictive control
n_episodes = 100
n_timesteps = 25
n_samples = 1
discrete_betas = jnp.linspace(beta_min / R, beta_max / R, R)
alphas = 1. - discrete_betas
snr = 0.075

def get_mean_std(filename,var_name):
    with open(filename, 'rb') as handle:
        data = pickle.load(handle)
        var = data[var_name][:,:26,:]
        length = var.shape[2]
        mean = var.mean(axis=(0,1)).reshape((-1,length))
        std = var.std(axis=(0,1)).reshape((-1,length))

    return mean, std

def get_action_mean_std(filename):
    action = jnp.array(np.load(filename))
    length = action.shape[-1]
    action_mean = action.mean(axis=(0,1)).reshape((length,))
    action_std = action.std(axis=(0,1)).reshape((length,))
    
    return action_mean, action_std

obs_mean, obs_std = get_mean_std("data/obj_sigmoid_actions.pickle",'obs')
post_sigmoid_action_mean, post_sigmoid_action_std = get_mean_std("data/obj_sigmoid_actions.pickle",'actions')
pre_sigmoid_action_mean, pre_sigmoid_action_std = get_mean_std("data/obj",'actions')

if model_type == 'empowerment' or model_type == 'VAE':
    z_action_mean, z_action_std = get_action_mean_std(model_type + '/new_1daction.npy')
if model_type == 'synergy_manifold':
    manifold_action_mean, manifold_action_std = get_action_mean_std('empowerment/new_6daction.npy')
if model_type == 'VAE_6d':
    VAE_6daction_mean, VAE_6daction_std = get_action_mean_std('VAE/new_6daction.npy')
if model_type == 'state_only':
    post_args = posterior_args()
    with open("empowerment/params_posterior.pickle", "rb") as input_file:
        params_pos = pickle.load(input_file)
        posterior = infer(h_dims_posterior = post_args.h_dims_dynamics,
                          control_variables = post_args.control_indx)
        jit_posterior = jit(posterior.apply)

if model_type == 'empowerment' or model_type == 'state_only':
    with open("dynamics/params_dynamics_0822.pickle", "rb") as input_file:
        params_dyn = pickle.load(input_file)
        dynamics_model = dynamics(args.h_dims_dynamics, args.control_indx)
        tx = optax.chain(optax.adamw(learning_rate = args.learning_rate,
                            b1 = args.adam_b1,
                            b2 = args.adam_b2,
                            eps = args.adam_eps,
                            weight_decay = args.weight_decay),
                            optax.clip_by_global_norm(args.max_grad_norm),
                            optax.ema(decay = args.ema_decay))
        state_dynamics = train_state.TrainState.create(apply_fn = dynamics_model.apply, params = params_dyn['params'], tx = tx)

    my_precoder = precoder(post_sigmoid_action_mean.reshape(6), post_sigmoid_action_std.reshape(6))
    jit_my_precoder = jit(my_precoder.apply)

if model_type == 'VAE':

    with open("VAE/params_VAE.pickle", "rb") as input_file:
        params_VAE = pickle.load(input_file)['params']

    args_VAE = VAE_args()
    decoder_model = MLP_Gaussian(h_dims = args_VAE.h_dims_decoder,
                                 out_dim = 6)
    jit_decoder = jit(decoder_model.apply)

if model_type == 'inverse_dynamics':

    with open("inverse_dynamics/params_inverse_dynamics_0822.pickle", "rb") as input_file:
        params_inverse_dynamics = pickle.load(input_file)

    args_inverse_dynamics = inverse_dynamics_args()
    inverse_dynamics_model = MLP_Gaussian(h_dims = args_inverse_dynamics.h_dims_inverse_dynamics,
                                          out_dim = 6)
    jit_inverse_dynamics = jit(inverse_dynamics_model.apply)

observations = []
rewards = []
truncateds = []
terminateds = []
total_info = []
z_actions = []
actions = []

env = gym.make('myoElbowPose1D6MRandom-v0')
env.np_random = np.random.default_rng(seed=0) # set environemnt seed so the initial state of each episode is the same across all methods
for episode in range(n_episodes):
    observation, info = env.reset()
    print(episode)
    frames = []
    for _ in range(n_timesteps):
        # normalise obs
        obs_norm = (observation[None,:] - obs_mean) / obs_std

        s0 = obs_norm

        # get action
        rng, step_rng = random.split(rng)
        unguided_sample, _ = reverse_sde(step_rng, args.horizon, n_samples, drift, dispersion, state_s.apply_fn, state_s.params, train_ts, snr, alphas, s0, args.transition_dim)
        
        if model_type == 'empowerment' or model_type == 'VAE':
            norm_z_action = unguided_sample[0,0,9:]

            # un-normalise action
            z_action = np.array((norm_z_action * z_action_std + z_action_mean))
            z_actions.append(z_action)
        
        if model_type == 'state_only':
            z_mean, z_log_var = jit_posterior({'params': params_pos['params']['posterior_model']}, unguided_sample[0,0,:9], unguided_sample[0,1,post_args.control_indx]) # unguided_sample[0,0,:9] = s0
            z_action = z_mean
            z_actions.append(z_action)

        if model_type == 'empowerment' or model_type == 'state_only':
            # get low-level action
            synergy = jit_my_precoder({}, obs_norm[0,:], state_dynamics)
            action = synergy @ z_action
        
        if model_type == 'VAE':
            a_mean, a_log_var = jit_decoder({'params': params_VAE['params']['decoder']}, z_action)
            # un-normalise action, where action is the mean of the decoder distribution
            action = a_mean*pre_sigmoid_action_std.squeeze() + pre_sigmoid_action_mean.squeeze()
            # sigmoided_action = a_mean*action_std.squeeze() + action_mean.squeeze()
            # action = (jnp.log(sigmoided_action) - jnp.log(1 - sigmoided_action))/5.+0.5 # inverse of nn.sigmoid((data['actions']-0.5)*5.)
        
        if model_type == 'synergy_manifold':
            norm_action = unguided_sample[0,0,9:]
            action = np.array((norm_action * manifold_action_std + manifold_action_mean))
        
        if model_type == 'VAE_6d':
            norm_action = unguided_sample[0,0,9:]
            norm_action = np.array((norm_action * VAE_6daction_std + VAE_6daction_mean))
            action = np.array((norm_action * pre_sigmoid_action_std.squeeze() + pre_sigmoid_action_mean.squeeze()))
            
        if model_type == 'inverse_dynamics':
            a_mean, a_log_var = jit_inverse_dynamics(params_inverse_dynamics['params'], jnp.concatenate((unguided_sample[0,0,:9], unguided_sample[0,1,:9]),axis=-1))
            action = a_mean*pre_sigmoid_action_std.squeeze() + pre_sigmoid_action_mean.squeeze()

        if model_type == 'baseline':
            norm_action = unguided_sample[0,0,9:]
            action = np.array((norm_action * pre_sigmoid_action_std.squeeze() + pre_sigmoid_action_mean.squeeze()))
        
        actions.append(action)

        observation, reward, terminated, truncated, info = env.step(np.array(action))

        observations.append(observation)
        rewards.append(reward)
        truncateds.append(truncated)
        terminateds.append(terminated)
        total_info.append(info)

        frame = env.sim.renderer.render_offscreen(
            width=400,
            height=400,
            camera_id=0)
        frames.append(frame)

    # https://github.com/scikit-video/scikit-video/issues/98
    # conda install -c menpo ffmpeg
    # pip install ffmpeg-python
    if frames:
        skvideo.io.vwrite(model_type + '/MPC/videos_z/video{}.mp4'.format(episode), np.array(frames), outputdict={"-pix_fmt": "yuv420p"})
        print(f"Video saved: episode {episode}")

env.close()

# save all the information
np.save(model_type + '/MPC/observations0919.npy', np.array(observations))
np.save(model_type + '/MPC/rewards0919.npy', np.array(rewards))
np.save(model_type + '/MPC/truncateds0919.npy', np.array(truncateds))
np.save(model_type + '/MPC/terminateds0919.npy', np.array(terminateds))
np.save(model_type + '/MPC/total_info0919.npy', np.array(total_info))
np.save(model_type + '/MPC/actions0919.npy', np.array(actions))
np.save(model_type + '/MPC/z_actions0919.npy', np.array(z_actions))

# examine predictive control 
# reward
plt.figure()
rwd_result = np.array(rewards).reshape((-1,n_timesteps)).T
plt.plot(rwd_result, 'b', alpha = 0.1)
plt.title(f'{model_type} reward')
plt.xlabel('time steps')
plt.ylabel(f'{model_type} reward')
plt.savefig(model_type + '/MPC/reward.pdf')

rewards = np.array(rewards).reshape((100,25))
average_rwd = np.average(np.sum(rewards,axis=1))
std_rwd = np.std(np.sum(rewards,axis=1))
np.save(model_type + '/MPC/cum_rwd_mean_std.npy', [average_rwd, std_rwd])
print(f"{model_type} average rewards: {average_rwd}")
print(f"{model_type} std rewards: {std_rwd}")

# error
plt.figure()
pos_err = []
for i in range(n_episodes*n_timesteps):
    pos_err.append(total_info[i]['obs_dict']['pose_err'])
pos_err = np.array(pos_err).reshape((-1,n_timesteps))
plt.plot(pos_err.T, 'b', alpha=0.1)
plt.title(f'{model_type} elbow joint angle error')
plt.xlabel('time steps')
plt.ylabel('error')
plt.savefig(model_type + '/MPC/error.pdf')

# ratio of solved
solved = np.array([dic['solved'] for dic in total_info]).reshape((n_episodes, n_timesteps))

average_solved = np.sum(solved) / (n_episodes * n_timesteps)
np.save(model_type + '/MPC/average_solved.npy', average_solved)
print(f"{model_type} average_solved: {average_solved}")
std_solved = np.std(np.average(solved,axis=-1))
np.save(model_type + '/MPC/std_solved.npy', std_solved)
print(f"{model_type} std_solved: {std_solved}")

stability = [np.all(episode[np.argmax(episode):]) for episode in solved]
average_stability = np.sum(stability) / n_episodes
np.save(model_type + '/MPC/average_stability.npy', average_stability)
print(f"{model_type} average_stability: {average_stability}")

# measure of effort
muscles = np.array(observations)[:,3:]
effort = np.linalg.norm(np.array(muscles), axis=-1).reshape((n_episodes, n_timesteps))
average_effort = np.average(effort)
std_effort = np.std(np.average(effort,axis=-1))
np.save(model_type + '/MPC/effort_mean_std.npy', [average_effort, std_effort])
print(f"{model_type} average_effort: {average_effort}")
print(f"{model_type} std_effort: {std_effort}")